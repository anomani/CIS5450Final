# -*- coding: utf-8 -*-
"""ProjectNotebookRecent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iKG2bMg6mCi5PL9seXEbLv-giAFZj47L

**CIS 5450 Final Project - Churn Prediction**

Adam Nomani, Areeb Alam, Eleonora Tomazou

## **Part 1: Introduction**

In our final project, our objective was to create a predictive model about whether a customer will churn or not based on specific variables explaining their behavior. Our ultimate objective was to understand the significance of various features on the churn rate of customers.

To this end, we selected a dataset with data from the entertainment industry. We found churn rate to be a particularly interesting topic in the entertainment industry because it is a contractual business with discrete payments made monthly or annually (subscriptions). Therefore, looking at the behavior of customers could help us yield insights regarding the factors that contribute to customers canceling their subscriptions. The analysis that we have conducted allows businesses in the industry to understand the various factors and take proactive approaches to retain customers, increase customer lifetime value, and stabilize their revenue stream.

## **Part 2: Imports and File Setup**

We initially imported all the Python libraries and functions we used throughout the project. This includes tools for data handling, data preprocessing, machine learning, and evaluation metrics.
"""

# Import necessary libraries for data handling and visualization
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats.mstats import winsorize
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import roc_auc_score
import plotly.express as px
!pip install pandasql
from pandasql import sqldf
from scipy.stats import spearmanr
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

"""## **Part 3: Importing Data**

Our dataset originates from Kaggle, so we directly downloaded it from there.
"""

# Import Google Colab's drive module and mount the Google Drive to access files stored there
from google.colab import drive
drive.mount('/content/drive')

# Create a directory for Kaggle configurations if it doesn't already exist
!mkdir -p ~/.kaggle

# Copy the Kaggle API key to the appropriate location to access Kaggle datasets
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

# Download the dataset for customer churn prediction from Kaggle
!kaggle datasets download -d safrin03/predictive-analytics-for-customer-churn-dataset

# Unzip the downloaded dataset for use
!unzip /content/predictive-analytics-for-customer-churn-dataset.zip

"""## **Part 4: Understanding our Data and Exploratory Analysis**

### **Part 4.1: Understanding our Data and Data Cleaning**

Our dataset came with a CSV called data_descriptions which includes columns of our data with descriptions and types. Therefore, we initially looked at it closely to understand the structure, content, and type of the data we are dealing with. The descriptions also allowed us to understand which columns are categorical and which are numerical.
"""

# Load a CSV file that describes the data, including column names, descriptions, and data types
data_description = pd.read_csv("data_descriptions.csv")
data_description

"""The dataset includes a mix of features related to a user's account and preferences, such as account age, monthly charges, and content preferences, as well as an identifier and a target variable, 'Churn'. Additionally, we observed that our dataset contains features that accumulate over time, such as Total Charges, as well as features averaged monthly, like Monthly Charges. This is important as although total charges represent the accumulation of monthly charges, the frequency and amount of monthly payments directly affect customer psychology, thus significantly influencing churn decisions. Looking at the features and their relationships was important to the predictive and interpretable models we created and the feature engineering we conducted.

Although the dataset already contained a train and a test CSV file, the test CSV file did not include the target variable, 'Churn'. Therefore in our analysis, we will only be using the train.csv file and performing our own train-test split off of this.
"""

# Load the main dataset used for the analysis into a DataFrame
test_df = pd.read_csv("test.csv")
train_df = pd.read_csv("train.csv")
train_df.head()

# Check data types of all columns in the DataFrame
train_df.dtypes

"""After understanding the structure of our dataset we wanted to examine whether there are any null values that we should handle. As our output below informed us, there are no null values in our dataset."""

# Calculate the number of null values for each column in the train_df DataFrame
null_values = train_df.isnull().sum()
print(null_values)

"""Next, we wanted to check how balanced our data is in terms of the target variable "Churn"."""

# Calculate and print the distribution of values in the 'Churn' column to check for balance
churn_balance = train_df['Churn'].value_counts()

# Calculate the percentage of each unique value
churn_percentage = train_df['Churn'].value_counts(normalize=True) * 100

# Display the balance and the percentage
print(churn_balance)
print(churn_percentage)

"""Our dataset is not balanced as approximately 81% of the data belongs to the negative class (Churn = "No"), while only about 18% of the data belongs to the positive class (Churn="Yes"). This analysis informs us that we will have to deal with class imbalance to improve the performance of our machine-learning models.

Categorical Variables: Many of our variables are categorical. As part of the data-cleaning process, we decided to print out all the unique values for each variable to understand the specific categories within each feature. Additionally, this was done to determine if any further data cleaning was necessary.
"""

# Display unique values in the categorical columns
print(train_df['SubscriptionType'].unique())
print(train_df['PaymentMethod'].unique())
print(train_df['SubtitlesEnabled'].unique())
print(train_df['ContentType'].unique())
print(train_df['DeviceRegistered'].unique())
print(train_df['GenrePreference'].unique())
print(train_df['Gender'].unique())
print(train_df['ParentalControl'].unique())
print(train_df['PaperlessBilling'].unique())
print(train_df['MultiDeviceAccess'].unique())

"""Upon examining the unique values of each category, we observed that the dataset was already cleaned in terms of inputs for each category. Each level of the category is clear, unambiguous, and consistent.

### **Part 4.2: Exploratory Data Analysis**

Numerical Variables: For numerical variables, we computed the basic descriptive statistics to understand the central tendencies, variability, and overall distribution.
"""

# Display basic descriptive statistics for all numerical columns in the DataFrame
train_df.describe()

"""*Total Charges* : The average of the TotalCharges is around 750.74 dollars, but varies significantly, with a standard deviation of 523.07 dollars. The big dispersion in the Total Charges most probably suggests the diverse customer base in terms of how long the customers have been with the business. Additionally, it might also reflect the different levels of engagement with the business. The big spread also suggests the presence of outliers, which we will examine later.

Different variables also point out the diversity in the customer base. For example, the average ViewingHoursDuration is about 92.26 minutes and the standard deviation is 50.51 minutes, a big proportion of the mean. Similarly, the average ContentDownloadsPerMonth is 24.5 and the standard deviation is 14.42, suggesting differences in content engagement.

The significant diversity suggests that there could be distinct segments in the customer base and that clustering analysis could be conducted to group customers based on similar usage patterns and preferences.

Based on our findings after examining the summary statistics above, we decided to create a histogram showing the distribution of the TotalCharges variable.
"""

# Create a histogram of the 'TotalCharges' column from the train_df DataFrame
plt.figure(figsize=(10, 6))
plt.hist(train_df['TotalCharges'], bins=30, color='blue', alpha=0.7)
plt.title('Histogram of Total Charges')
plt.xlabel('Total Charges')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""The histogram of Total Charges shows a right-skewed distribution, indicating that a larger number of customers have lower total charges, while fewer customers have very high total charges. The peak of the histogram occurs in the lower range of charges, with frequency gradually decreasing as the charges increase. The high standard deviation relative to the mean observed earlier is consistent with the spread and tail seen in the histogram. In terms of churn, it will be important to understand the segments that contribute to this tail and the churn rates.

By looking at the distributions of AccountAge and MonthlyCharges we will be able to understand whether the right-skewed distribution is attributed to one of the two or a combination of the two.
"""

# Set up the matplotlib figure
f, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=False)

# Plot with color overlay for Churn on Account Age
sns.histplot(data=train_df, x='AccountAge', hue='Churn', multiple='stack', palette="light:m_r", edgecolor=".3", linewidth=.5, ax=axes[0])
axes[0].set_title('Distribution of Account Age with Churn Overlay')
axes[0].set_xlabel('Account Age (months)')
axes[0].set_ylabel('Frequency')

# Plot with color overlay for Churn on Monthly Charges
sns.histplot(data=train_df, x='MonthlyCharges', hue='Churn', multiple='stack', edgecolor=".3", linewidth=.5, ax=axes[1])
axes[1].set_title('Distribution of Monthly Charges with Churn Overlay')
axes[1].set_xlabel('Monthly Charges ($)')

plt.tight_layout()
plt.show()

"""By looking at the distributions of Account Age and Monthly Charges we see that they are both fairly uniform, meaning that they are evenly distributed. This informs us that the skewed distribution of the Total Charges variable likely stems from the multiplicative interaction between "Account Age" and "Monthly Charges." Long-standing customers with higher monthly charges can significantly skew the distribution of total charges, reflecting a compounding effect that underscores the importance of considering both factors and potentially their interaction when predicting churn.

We have overlayed churn as a color dimension to visualize how churn varies across the different values of Account Ages and Monthly Charges. We observe that as Account Age increases the likelihood of churn decreases. In contrast, as Monthly Charges increase, the likelihood of churn increases.
"""

# Create a scatter plot with 'AccountAge' on the x-axis, 'TotalCharges' on the y-axis, and color-coded by 'MonthlyCharges'
fig = px.scatter(train_df, x='AccountAge', y='TotalCharges',
                 color='MonthlyCharges', color_continuous_scale=px.colors.sequential.Viridis,
                 labels={'MonthlyCharges': 'Monthly Charges'},
                 title='Account Age vs. Total Charges Colored by Monthly Charges')
fig.show()

"""To better see the relationship between Total Charges with Account Age and Monthly Charges, we created a scatter plot with Total Charges on the y-axis, Account Age on the x-axis, and the Monthly Charges overlayed as a color dimension. The graph clearly shows the cumulative effect on Total Charges revealing a broadening pattern like a megaphone. The range of Total Charges at high Account Ages is much greater than the range at lower Account Ages.

Continuing with our analysis, we decided to explore some other of the features in our dataset. Below we have plotted the ViewingHoursWeek on the y-axis and the subscription type as a categorical variable on the x-axis. Additionally, for each combination, we have one box plot for Churn = "Yes" and one box plot for Churn = "No".
"""

# Create a box plot with 'SubscriptionType' on the x-axis, 'AverageViewingDuration' on the y-axis, and color-coded by 'Churn'
fig = px.box(train_df, x='SubscriptionType', y='AverageViewingDuration', color='Churn',
             labels={'Churn': 'Churn Status'},
             title='Viewing Hours Per Week by Subscription Type with Churn Overlay')
fig.show()

"""Surprisingly, the subscription type does not seem to greatly affect either the churn rate or the viewing hours per week. However, we do observe that the statistics of the viewing hours per week are different for Churn = "Yes" and Churn = "No". The maximum and minimum viewing hours per week are similar between the two classes but the median, lower, and upper quartiles are all much lower for Churn = "No" than Churn = "Yes".

A similar conclusion was also made by looking at the box plot of ContentDownloadsPerMonth varying across the Subscription Type.

Based on our previous results, on the insignificance of the subscription type, we decided to see how different variables depended on the subscription type. To this end, we used SQL. We decided to use SQL for its efficiency in querying and analyzing large datasets, as well as its intuitive syntax.
In the block of code below we group the rows by SubscriptionType and calculate the TotalSubscriptions in each, the positive cases of Churn ('Churn' = Yes), and the average of MonthlyCharges.
"""

# Selecting SubscriptionType and aggregating relevant metrics for analysis
q = """
SELECT SubscriptionType, COUNT(*) as TotalSubscriptions,
       SUM(CASE WHEN Churn == 1.0 THEN 1 ELSE 0 END) as TotalChurned,
       AVG(MonthlyCharges) as AvgMonthlyCharges,
       AVG(AverageViewingDuration) as AveragedAvgViewingDuration,
       AVG(UserRating) as AvgUserRating
FROM train_df
GROUP BY SubscriptionType
"""

def pysqldf(q):
    return sqldf(q, globals())

# Running the SQL query
subscription_aggregation = pysqldf(q)
print(subscription_aggregation)

"""By grouping the dataset based on the Subscription Type, we observed that most of the variables are equal across the 3 subscription types. This gives us more information about the SubscriptionType variable as we would normally expect AvgMonthlyCharges, AverageViewingDuration, and UserRating to vary across the subscription types. The output confirms the absence of correlations between subscription type and the other variables in the dataset and suggests that the differences in these plans may not be primarily driven by factors such as price or service features. Although we observe a small difference in churn rates between the Basic and Standard subscription types, the number of churned customers in the Premium subscription tier is noticeably lower, suggesting that SubscriptionType might be an important variable in predicting churn."""

q = """
SELECT
    CAST(UserRating AS INTEGER) AS UserRating,
    SUM(CASE WHEN Churn == 1.0 THEN 1 ELSE 0 END) as TotalChurned,
    AVG(ContentDownloadsPerMonth) AS AvgDownloadsPerMonth,
    AVG(AverageViewingDuration) AS AvgViewingDuration,
    AVG(AccountAge) AS AvgAccountAge,
    AVG(MonthlyCharges) AS AvgMonthlyCharges
FROM
    train_df
GROUP BY
    CAST(UserRating AS INTEGER)
ORDER BY
    CAST(UserRating AS INTEGER);
"""

def pysqldf(q):
    return sqldf(q, globals())

# Running the SQL query
userrating_aggregation = pysqldf(q)
print(userrating_aggregation)

"""We performed a similar analysis for the UserRating after casting it as an integer. As with the SubscriptionType, we did not discern any differences in the averages across different variables in our dataset including DownloadsPerMonth, ViewingDuration, AccountAge and MonthlyCharges. This also suggests that rating is likely uncorrelated with the other variables of the dataset. The TotalChurned varied throughout the different ratings but not significantly, suggesting that UserRating is potentially not as sgnificant as other variables in predicting Churn."""

# Creating a bar plot to visualize the relationship between the number of support tickets per month and churn rate
plt.figure(figsize=(10, 5))
sns.barplot(x='SupportTicketsPerMonth', y='Churn', data=train_df)
plt.title('Churn Rate by Number of Support Tickets Per Month')

plt.show()

"""It is evident in the plot above that as the number of support tickets increases, so does the probability of churn. It is interesting to note that the relationship appears to be close to linear. This suggests that the relationship between the number of support tickets per month and the probability of churn may be adequately captured by a linear model like logistic regression. Additionally, we observe that the range of the rate of churn is about 0.24 - 0.14 = 0.10. This indicates significant variability in the churn rate, which could be useful for our predictive model of churn.

Before proceeding in analyzing our data and fitting predictive models it is important to check for feature correlations. High correlations in our features can result in unreliable estimates in logistic regression, as well as reduce the clarity of feature importance that we analyze later. For this reason, we are using a correlation matrix heatmap below to visualize the correlations.
"""

# Extract the numerical and categorical clumns
numerical_cols = ['AccountAge', 'MonthlyCharges', 'TotalCharges', 'ViewingHoursPerWeek', 'AverageViewingDuration', 'ContentDownloadsPerMonth', 'UserRating', 'SupportTicketsPerMonth', 'WatchlistSize', 'Churn']
numerical_df = train_df[numerical_cols]

# Calculate and display a correlation matrix as a heatmap for selected numerical columns
correlation_matrix = numerical_df.corr()

plt.figure(figsize=(10, 8))  # Adjust the figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix Heatmap')

plt.show()

"""The variables in our dataset are very weakly correlated, with most of the correlations being 0. A few of the correlations are around -0.1 to -0.2. The highest correlation is between Total Charges and Account Age and is around 0.8."""

# Print value counts for specific categorical columns to understand their distributions
for column in ['SubscriptionType', 'PaymentMethod', 'Gender', 'ContentType', 'MultiDeviceAccess']:
    print(train_df[column].value_counts())

"""We see that overall the categories are balanced with an equal number of instances across each level. The dataset is therefore well-structured in terms of representation, and specific measures to balance these categories further are not necessary. We should focus on balancing the target variable, Churn, as we saw that there is a big class imbalance.

Now, we can look at correlations of categorical variables with Churn using Spearman Rank Order
"""

# Calculating and printing Spearman correlation between 'SubscriptionType' and 'Churn'
corr, pval = spearmanr(train_df['SubscriptionType'], train_df['Churn'])
print('Correlation between SubscriptionType & Churn: ' + str(corr))

# Calculating and printing Spearman correlation between 'GenrePreference' and 'Churn'
corr, pval = spearmanr(train_df['GenrePreference'], train_df['Churn'])
print('Correlation between GenrePreference & Churn: ' + str(corr))

# Calculating and printing Spearman correlation between 'Gender' and 'Churn'
corr, pval = spearmanr(train_df['Gender'], train_df['Churn'])
print('Correlation between Gender & Churn: ' + str(corr))

"""The categorical variables we selected (SubscriptionType, GenrePreference and Gender) do not seem to have a high correlation with the target variable, Churn.

### **Part 4.3: Outliers**

As part of our EDA, it is essential to check for any significant outliers that might bias our analysis and models. The method we have selected to detect outliers is the Interquartile Range Method (IQR). Apart from its simplicity, we have selected this method to detect outliers because it is good for skewed distributions. We predicted that some of our variables could be skewed as they are variables describing duration. For example, total charges accumulate over time and so we expect this variable to have a heavy skew. This is confirmed in our outlier detection below.
"""

# Selecting numerical features by including columns with data types 'int64' and 'float64'
numerical_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Filtering out the target variable 'Churn' from the list of numerical features
numerical_features = [feature for feature in numerical_features if feature != 'Churn']

# Iterate through each numerical feature to detect outliers
for variable in numerical_features:
  # Calculate the first and third quartiles
  q1, q3 = train_df[variable].quantile([0.25, 0.75])
  # Calculate the interquartile range (IQR)
  iqr = q3 - q1
  # Calculate the lower and upper limits for outliers detection
  lower_limit = q1 - 1.5 * iqr
  upper_limit = q3 + 1.5 * iqr

  # Filter outliers based on the lower and upper limits
  outliers = train_df[(train_df[variable] < lower_limit) | (train_df[variable] > upper_limit)]

  print(f"{variable} - Outliers Count: {len(outliers)}")

"""The IQR method also allows for a visualization using a box plot."""

# Creating a box plot for the 'TotalCharges' column, dropping any NA values
plt.figure(figsize=(5, 3))
plt.boxplot(train_df['TotalCharges'])
plt.title('Box Plot of Total Charges')
plt.ylabel('Total Charges')
plt.xticks([1], ['Total Charges'])
plt.grid(True)
plt.show()

"""**Outlier Transformation**

To deal with outliers we will use winsorising to change the values of the outliers to the 5th and 95th percentile of the data. Using this method we can limit the effect of outliers without completely removing them.
"""

# Winsorizing at the 5th and 95th percentiles
train_df['Total Charges_winsorized'] = winsorize(train_df['TotalCharges'], limits=[0.05, 0.05])

# Plotting to visualize the transformation effect
train_df['TotalCharges'].plot(kind='hist', bins=50, alpha=0.5, title='Before and After Winsorization')
train_df['Total Charges_winsorized'].plot(kind='hist', bins=50, alpha=0.5)
plt.legend(['Original', 'Winsorized'])
plt.show()

# Dropping the original TotalCharges column
train_df.drop(columns=['TotalCharges'], inplace=True)

"""## **Part 5:** **Data Pre Processing & Feature Engineering**

### **5.1 One-hot Encoding**

Many of the features in our dataset are categorical. In order to perform logistic regression, we used OneHotEncoding which transformed the categorical variables into numerical ones.

We firstly examined which of our variables are numerical and which are categorical.
"""

# Get numerical features from the dataframe
numerical_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Get numerical features from the dataframe
numerical_features = [feature for feature in numerical_features if feature != 'Churn']
# Get categorical features from the dtaframe
categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()

"""Firstly, we used One-hot encoding for the binary features. This created just one column which had a value of 0 for the baseline category and a value of 1 for the alternative category."""

# Create copies of the train and test dataframes
binary_train_df = train_df.copy()
binary_test_df = test_df.copy()
# Identify binary columns
binary_columns = [column for column in train_df.columns if train_df[column].nunique() == 2]
# Initialize the OneHotEncoder
encoder = OneHotEncoder(drop='if_binary', dtype=float)
# Fit the encoder to the binary columns of the train dataframe and transform them into one-hot encoded format
binary_train_df[binary_columns] = encoder.fit_transform(train_df[binary_columns]).toarray()

"""Using One-hot encoding for the unordered features in the dataset enabled us to create (# levels - 1) extra columns for each of the features."""

# List of the unordered categorical features
unordered_features = ['ContentType', 'DeviceRegistered', 'GenrePreference', 'PaymentMethod', 'SubscriptionType']
# Copy of the binary_train_df
encoded_train_df = binary_train_df.copy()
# Initialize the OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse=False)
# Extract the unordered categorical data from the encoded_train_df
categorical_data = encoded_train_df[unordered_features]
# Fit the encoder to the categorical data and transform it into one-hot encoded format
encoded_data = encoder.fit_transform(categorical_data)
# Get the feature names of the encoded data
encoded_feature_names = encoder.get_feature_names_out(input_features=unordered_features)
# Create a DataFrame from the encoded data with the appropriate column names
encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names, index=encoded_train_df.index)
# Drop the original unordered categorical features from the encoded_train_df
encoded_train_df.drop(columns=unordered_features, inplace=True)
# Concatenate the encoded dataframe with the original encoded_train_df along the columns axis
encoded_train_df = pd.concat([encoded_train_df, encoded_df], axis=1)

"""We decided to drop the CustomerID feature as it does not predict churn"""

encoded_train_df = encoded_train_df.drop('CustomerID', axis=1)

"""### **Part 5.2: Train - Test split**

We split our data into a train and test set. Specifically, we reserved 20% of the data for testing and the remaining 80% for training. This ensures that we can train our models in one portion of the data (training set) and evaluate the performance on a separate, unseen portion of the data (test set). We selected the specific 80/20 split because it provides a substantial amount of data for the training procedure and also keeps an appropriate amount of samples for testing.
"""

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    encoded_train_df.drop('Churn', axis=1),  # Features: dropping the 'Churn' column
    encoded_train_df['Churn'], # Target variable: 'Churn' column
    test_size=0.2 # 20% of the data will be used for testing
)

"""### **Part 5.3: Standardization**

Before fitting any model, we standardized the numerical features to make sure that the features contribute equally to the model's training process. Specifically, we used the StandardScaler() from scikit-learn. The StandardScaler() computes the mean and standard deviation for each numerical feature in the training set. It then adjusts the training data to have a mean of zero and a standard deviation of one. We then applied the same scaler to the test data.
"""

# Initializing the StandardScaler
scaler = StandardScaler()
# Fitting the scaler to the numerical features in the training set
scaler.fit(X_train[numerical_features])
# Transforming the numerical features in the training set
X_train[numerical_features] = scaler.transform(X_train[numerical_features])
# Transforming the numerical features in the testing set using the same scaler
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

"""### **Part 5.4: Balance classes**

Through our EDA we realized that our data is imbalanced, with the churn = 'Yes' (positive) instances far exceeding the churn = 'No' (negative) instances. For this reason, we used SMOTE to balance the two classes. This will result in more accurate models as it helps to handle the problem of overfitting that can occur when the model is overly trained in the majority class.

We chose SMOTE because it works by creating synthetic samples rather than over-sampling with replacement, which can lead to overfitting by making an exact sample of the minority class. It should be noted that later (after looking at the logistic regression results) we examine another method of balancing classes, k-means clustering.
"""

# Initializing the SMOTE with a random state
smote = SMOTE(random_state=42)
# Applying SMOTE to the training data to balance the classes
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

"""### **Part 5.5: PCA and subsequent K-means Clustering**

A common practice in marketing is to first perform PCA analysis in order to reduce the number of features to only a few interpretable factors and then to perform clustering on the principal components in order to see whether there exist segments in the markets in terms of their behavior in the two underlying factors. This will help us better understand our dataset as through PCA we will understand the main directions of variation and transform the high-dimensional data into a few features that preserve the most information. Clustering on these can reveal inherent groupings that represent different market segments or consumer behaviors.

 Additionally, we will also use this analysis to create a new feature: the cluster to which each data point belongs, as a categorical variable. This feature will be used in our logistic regression model. The new feature will enable the logistic regression to capture non-linear relationships.

 We also experimented with adding a continuous feature, which would be the distance of each data point from its respective cluster center. Essentially, interpreting the coefficient of this feature could also give insights into whether more "outlying" customers or "representative of the cluster" customers are more responsive. However, this feature did not improve the logistic regression accuracy. For simplicity purposes, we have only included the logistic regression model with the added categorical feature (cluster to which each data point belongs) that was described above.

### **Part 5.5.1: PCA**

We chose to retain n = 2 components for our PCA analysis. After experimenting, we found that these two components capture significant orthogonal directions. This was evident from the correlations between the original variables and the components, details of which will be demonstrated later in the analysis. Keeping fewer components also enhanced the interpretability of the results and our ability to create clear visual plots

We performed PCA in our balanced train set, *X_train_smote*.
"""

# Initializing PCA with 2 components
pca = PCA(n_components=2)
# Applying PCA to the training data after SMOTE
X_train_pca = pca.fit_transform(X_train_smote)

"""We printed out the feature loadings resulting from our PCA. Feature loadings are the correlations between the original variables (features) and the principal components. Each principal component is a linear combination of the original variables, and the feature loadings indicate the contribution of each original variable to that component. Looking at the feature loadings we were able to understand the underlying structure in the data and which variables are most relevant in explaining the variability observed in the dataset."""

# Setting display option to show all columns
pd.set_option('display.max_columns', None)
# Creating a DataFrame to store feature loadings from PCA
feature_loadings = pd.DataFrame(pca.components_, columns=X_train_smote.columns)
print(feature_loadings)

"""Based on the feature loadings with the original variables, we can interpret what the principal components capture. It is common in marketing to try to name the components according to which original variables they "load on" (correlate with) well. Looking at the results above, we could conclude that Component 1 could be the **"Customer Lifetime Value"**, since it captures the overall value or importance of a customer based on their account age, total charges, and monthly charges. On the other hand, Component 2 could be named **"Engagement vs Cost Sensitivity"**. This factor reflects customers who may be newer (low account age)  but are affected by high monthly charges and tend to spend many hours viewing, suggesting they may be heavy users or subscribers to premium services.

Now that we have understood what the two principal components capture, we will cluster our dataset based on the two components. This will allow us to understand the segments that exist in terms of the two components. Additionally, we will print out the percentage of people who churned in each cluster. In this way, we will be able to understand how our target variable changes across the different segments we have created.

### **Part 5.5.2: Clustering on the Principal Components**
"""

# Initializing KMeans with 4 clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(X_train_pca)
labels = kmeans.labels_

# Create a DataFrame from the PCA results and add cluster labels and churn data
cluster_data = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2'])
cluster_data['Cluster'] = labels
cluster_data['Churn'] = y_train_smote

# Reshaping labels for concatenation
labels = labels.reshape(-1, 1)

# Concatenate the cluster labels to the original feature matrix
X_train_with_clusters = pd.DataFrame(X_train_smote)
X_train_with_clusters['ClusterLabel'] = labels
X_train_with_clusters['ClusterLabel'] = pd.Categorical(X_train_with_clusters['ClusterLabel'])

# Compute centroids in the PCA space to plot
centroids = (kmeans.cluster_centers_)

"""We visualized the clusters by plotting them on a scale where the x-axis is Principal Component 1 and the y-axis is Principal Component 2. The plot further helps us with the interpretation of the clusters that exist in the market in terms of the two principal components."""

# Calculate the percentage of churned customers in each cluster
churn_rates = cluster_data.groupby('Cluster')['Churn'].mean() * 100
print("Churn rates by cluster:")
for cluster_num, churn_rate in churn_rates.items():
    print(f"Cluster {cluster_num + 1}: {churn_rate:.2f}% churn rate")

# Create a scatter plot
fig = px.scatter(cluster_data, x='PC1', y='PC2', color='Cluster', opacity=0.5,
                 title='PCA and K-means Clustering with Churn Rates',
                 labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},
                 hover_name='Cluster')

# Add centroids to the plot
fig.add_scatter(x=centroids[:, 0], y=centroids[:, 1], mode='markers', marker=dict(size=10, color='red'), name='Centroids')

# Add text labels for centroids
for i, centroid in enumerate(centroids):
    fig.add_annotation(x=centroid[0], y=centroid[1], text=str(i+1), showarrow=False)

# Show the plot
fig.show()

"""Looking at the percentage of people who churned in each cluster, we observe that the two principal components result in a significant change in the churn percentage. For example, cluster 3 could be described as people who are low on both principal component 1 ("customer lifetime value") and principal component 2 ("engagement vs cost sensitivity"). They have the highest churn rate of the 4 clusters (73.00%). On the other hand, cluster 2 has the lowest churn rate (25.59%). Cluster 2 is relatively high on both customer lifetime value and usage and spending patterns.

### **Part 5.5.3: Chosing the number of Clusters**

In order to choose the appropriate number of clusters, we created a plot with the mean value of the PCA components on the y-axis and the two PCA components as categorical variables on the x-axis. Looking at the plot, we compared the plots created from testing different numbers of clusters. It turned out that after 4 clusters, some of the clusters had very similar means. This suggested that 4 clusters were appropriate to characterize the data based on the two principal components.
"""

# Calculate means for each PCA component within each cluster
cluster_means = cluster_data.groupby('Cluster').mean()

# Transpose the DataFrame so that PCA components are on the x-axis
# and each row represents a cluster
cluster_means_transposed = cluster_means.T
cluster_means_transposed = cluster_means_transposed.drop('Churn')

# Plotting each cluster as a separate line
fig, ax = plt.subplots(figsize=(10, 6))
for i in range(len(cluster_means_transposed.columns)):
    ax.plot(cluster_means_transposed.index, cluster_means_transposed.iloc[:, i], marker='o', label=f'Cluster {i}')

plt.title('Mean Values of PCA Components by Cluster')
plt.ylabel('Mean Value of PCA Components')
plt.xlabel('PCA Components')
plt.grid(True)
plt.legend(title='Cluster')
plt.show()

"""In order to use the cluster labels as a new feature in our dataset, we need to encode them. We will use one-hot encoding as with the other categorical variables. In one-hot encoding, each category is represented by a binary vector, where each element corresponds to a category and is set to 1 if the data point belongs to that category and 0 otherwise. Below we follow the same procedure we followed earlier to encode the ClusterLabel variable."""

# Defining the feature representing the cluster labels
cluster_label_feature = ['ClusterLabel']

# Initialize the OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse=False)

# Extract the cluster label data from the X_train_with_clusters DataFrame
cluster_label_data = X_train_with_clusters[cluster_label_feature]

# Fit the encoder to the cluster label data and transform it into one-hot encoded format
encoded_data = encoder.fit_transform(cluster_label_data)

# Get the feature names of the encoded data
encoded_feature_names = encoder.get_feature_names_out(input_features=cluster_label_feature)

# Create a DataFrame from the encoded data with the appropriate column names
encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names, index=X_train_with_clusters.index)

# Drop the original cluster label feature from the X_train_with_clusters DataFrame
X_train_with_clusters.drop(columns=cluster_label_feature, inplace=True)

# Concatenate the encoded dataframe with the original X_train_with_clusters DataFrame along the columns axis
X_train_with_clusters = pd.concat([X_train_with_clusters, encoded_df], axis=1)

"""### **Part 5.6: Additional Notes**

Apart from the feature containing the cluster labels, we thought that the original features in the dataset were quite comprehensive. This is because the dataset includes both cumulative and non-cumulative variables, which are important as they can affect customer psychology, such as total charges. Therefore, we deemed it unnecessary to engineer additional features, except for interaction terms, which we will test later.

## **Part 6: Models**

### **6.1.1: Logistic Regression**

The first model we decided to implement is a logistic regression. Given that we are dealing with a classification problem (churn or not churn), logistic regression is appropriate because it provides a probability score for observations. We are utilizing the encoded, standardized, and balanced dataset that we have created. Logistic regression will also allow us to see which factors are most influential in predicting churn by looking at the regression coefficients. The goal is to get an interpretable model.

Limitations: Logistic Regression also has some limitations that might affect the performance of our model. Specifically, logistic regression assumes linearity between the independent variables and the log odds of Churn, which does not necessarily hold. Additionally, logistic regression also has limited flexibility given that it does not allow for interaction terms between the variables.

We decided not to perform PCA on all variables to reduce the dimensionality of our data in any of our models. This decision came from our observation that our features are not highly correlated. We therefore realized that PCA would not offer significant benefits in our case and would obscure the interpretability of the models (which is important for our ultimate objective of understanding the significance of different features in the churn rate of customers).

In the code below we directly use the train set that also contained our engineered feature (the cluster label - the cluster that each data point belongs to). This new feature turned out to only slightly increase the performance of the logistic regression (about a 1% increase in each of the performance metrics). However, we decided to keep it in our logistic regression model.

Additionally, we also decided to drop the Total Charges_winsorized column. This is because TotalCharges was highly correlated with AccountAge and MonthlyCharges. We realized that the newly engineered feature (cluster labels) captured the information on Total Charges given the high correlation of the principal components with TotalCharges. This resulted in very marginal increases in the performance metrics, especially the test set performance metrics.

In the code below, we transform the test set based on the PCA we fitted previously and add the cluster label feature.
"""

# Transforming the test set using the previously fitted PCA transformation
X_test_pca = pca.transform(X_test)

# Predicting cluster labels for the test set using KMeans
test_labels = kmeans.predict(X_test_pca)

# Convert test labels for concatenation
test_labels = test_labels.reshape(-1, 1)

# Getting feature names from the training DataFrame
feature_names = X_train.columns.tolist()

# Convert numpy array X_test to a DataFrame with these feature names
X_test_with_clusters = pd.DataFrame(X_test, columns=feature_names)

# Adding the predicted cluster label column to the test DataFrame
X_test_with_clusters['ClusterLabel'] = test_labels

"""We also need to one-hot encode the feature, as it is a categorical variable."""

# Convert 'ClusterLabel' to categorical data
X_test_with_clusters['ClusterLabel'] = pd.Categorical(X_test_with_clusters['ClusterLabel'])

# Define the feature representing the cluster labels
cluster_label_feature = ['ClusterLabel']

# Initialize the OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse=False)

# Extract the cluster label data from the X_test_with_clusters DataFrame
cluster_label_data = X_test_with_clusters[cluster_label_feature]

# Fit the encoder to the cluster label data and transform it into one-hot encoded format
encoded_data = encoder.fit_transform(cluster_label_data)

# Get the feature names of the encoded data
encoded_feature_names = encoder.get_feature_names_out(input_features=cluster_label_feature)

# Create a DataFrame from the encoded data with the appropriate column names
encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names, index=X_test_with_clusters.index)

# Drop the original cluster label feature from the X_test_with_clusters DataFrame
X_test_with_clusters.drop(columns=cluster_label_feature, inplace=True)

# Concatenate the encoded dataframe with the original X_test_with_clusters DataFrame along the columns axis
X_test_with_clusters = pd.concat([X_test_with_clusters, encoded_df], axis=1)

# Remove the 'Total Charges_winsorized' column from both X_test_with_clusters and X_train_with_clusters DataFrames
X_test_with_clusters.drop(columns=['Total Charges_winsorized'], inplace=True)
X_train_with_clusters.drop(columns=['Total Charges_winsorized'], inplace=True)

# Initialize LogisticRegression
lrg = LogisticRegression()

# Fit the logistic regression model on the training data
lrg.fit(X_train_with_clusters, y_train_smote)

# Predict outcomes for the train and test sets
lrg_train_predictions = lrg.predict(X_train_with_clusters)
lrg_test_predictions = lrg.predict(X_test_with_clusters)

# Calculate accuracy, recall, and precision metrics for train and test sets using stored predictions
lrg_train_acc = accuracy_score(y_train_smote, lrg_train_predictions)
lrg_test_acc = accuracy_score(y_test, lrg_test_predictions)
lrg_train_rec = recall_score(y_train_smote, lrg_train_predictions)
lrg_test_rec = recall_score(y_test, lrg_test_predictions)
lrg_train_pre = precision_score(y_train_smote, lrg_train_predictions)
lrg_test_pre = precision_score(y_test, lrg_test_predictions)

print('Regular Logistic Regression Performance')
print('---------------------------------------')
print(f'Training Accuracy: {lrg_train_acc*100:.2f}%')
print(f'Testing Accuracy: {lrg_test_acc*100:.2f}%')
print(f'Training Recall: {lrg_train_rec*100:.2f}%')
print(f'Testing Recall: {lrg_test_rec*100:.2f}%')
print(f'Training Precision: {lrg_train_pre*100:.2f}%')
print(f'Testing Precision: {lrg_test_pre*100:.2f}%')

# ROC AUC calculation using predicted probabilities if available
if hasattr(lrg, "predict_proba"):
    auc_train = roc_auc_score(y_train_smote, lrg.predict_proba(X_train_with_clusters)[:, 1])
    auc_test = roc_auc_score(y_test, lrg.predict_proba(X_test_with_clusters)[:, 1])
else:
    auc_train = roc_auc_score(y_train_smote, lrg.decision_function(X_train_with_clusters))
    auc_test = roc_auc_score(y_test, lrg.decision_function(X_test_with_clusters))

print('AUC Score')
print('---------')
print(f'Training AUC: {auc_train:.4f}')
print(f'Testing AUC: {auc_test:.4f}')

"""Looking at the results of the logistic regression, we see that the training accuracy is very close to the testing accuracy. This suggests that the model generalizes well on new data and that it does not overfit the training set. Overall, the **accuracy** of the model is high (around 68%). This also applies to the **recall** metric which is close for the train and test sets and is around 68%. However, the testing **precision** is a lot lower (32%) than the training precision (68%). This indicates that a considerable number of instances that are predicted as positive in the test set are negative. This was expected because although we handled the class imbalance in the train set, we did not do it in the test set to reflect the real world.

The most likely reason for the observed results is that the threshold optimized for a balanced scenario doesn't necessarily perform well in an imbalanced one.
"""

# Convert X_train_with_clusters to a DataFrame to ensure compatibility with coefficient indexing
X_train_with_clusters = pd.DataFrame(X_train_with_clusters)

# Get the column names from X_train_with_clusters
feature_names = X_train_with_clusters.columns.tolist()

# Create a DataFrame of coefficients with feature names as index and 'Coefficient' as the column name
coefficients = pd.DataFrame(lrg.coef_.T, index=feature_names, columns=['Coefficient'])

# Sort the coefficients DataFrame by absolute coefficient values in descending order
coefficients_sorted = coefficients.reindex(coefficients['Coefficient'].abs().sort_values(ascending=False).index)

# Print the sorted coefficients DataFrame
print(coefficients_sorted)

"""*   The highest coefficient in magnitude (-0.626160) was associated with the feature "AccountAge," indicating that older account ages are strongly correlated with a lower likelihood of churn. That is, a one-unit increase in the account age is associated with a 0.63 decrease in the log odds of churn.
*   AverageViewingDuration was the variable with the second highest coefficient in magnitude (-0.482998), suggesting that greater average viewing durations are strongly correlated with a lower likelihood of churn.
*  Our newly engineered feature (ClusterLabel) appears in as many columns as we have used one-hot encoding given it is a categorical variable. The coefficients indicate the extent to which being in cluster 1,2,3 increases the probability of churn relative to cluster 0, which is the baseline. The coefficients are however relatively small indicating that the influence of the cluster label on churn prediction is limited.
*   Other variables such as ContentDownloadsPerMonth, SubscriptionType_Premium, and MonthlyCharges also have high coefficients indicating their importance in the churn prediction. In contrast, features such as PaperlessBilling and MultiDeviceAccess had a small impact on the churn prediction.

Below we have ploted the Precision-Recall Curve in order why Recall is so much higher than Precision. We see that at the threshold of 0.5 Recall is much higher than Precision.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

probabilities = lrg.predict_proba(X_test_with_clusters)[:, 1]  # Get probabilities for the positive class

# Define a new threshold
custom_threshold = 0.3  # For example, this threshold is lower to catch more positives

# Apply the custom threshold to determine class labels
y_pred_custom = (probabilities >= custom_threshold).astype(int)

# Assuming 'probabilities' contains your model's predicted probabilities for the positive class
precision, recall, thresholds = precision_recall_curve(y_test, probabilities)

# Plotting the precision-recall curve
plt.figure(figsize=(8, 5))
plt.plot(thresholds, precision[:-1], "b--", label="Precision")
plt.plot(thresholds, recall[:-1], "g-", label="Recall")
plt.xlabel("Threshold")
plt.ylabel("Precision/Recall")
plt.legend(loc="best")
plt.title("Precision-Recall Curve")
plt.show()

"""Given this discrepancy we observed in the train and test set precision, we decided to take the following steps:

*  Perform Stratified k-fold cross-validation of the SMOTE balanced data to examine the performance of the logistic regression in the case where each fold maintains the same class distribution with each other.
*  Experiment with using undersampling instead of SMOTE for balancing and comparing the performance of the logistic regressions

### **6.1.1: Stratified K-Fold Cross-Validation**
"""

# Separate features (X) and target variable (y)
X = encoded_train_df.drop('Churn', axis=1)
y = encoded_train_df['Churn']

# Apply SMOTE to address class imbalance
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Define the number of splits for Stratified K-Fold cross-validation
n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
model = LogisticRegression(solver='liblinear')

# Lists to store metrics for each fold
train_accuracy_list, test_accuracy_list = [], []
train_recall_list, test_recall_list = [], []
train_precision_list, test_precision_list = [], []
train_auc_list, test_auc_list = [], []

# Perform cross-validation
for fold, (train_index, test_index) in enumerate(skf.split(X_resampled, y_resampled)):
    # Split data
    X_train_crossval, X_test_crossval = X_resampled.iloc[train_index], X_resampled.iloc[test_index]
    y_train_crossval, y_test_crossval = y_resampled.iloc[train_index], y_resampled.iloc[test_index]

    # Fit the model
    model.fit(X_train_crossval, y_train_crossval)

    # Predict on training and testing set
    train_predictions = model.predict(X_train_crossval)
    test_predictions = model.predict(X_test_crossval)

    # Store metrics
    train_accuracy_list.append(accuracy_score(y_train_crossval, train_predictions))
    test_accuracy_list.append(accuracy_score(y_test_crossval, test_predictions))
    train_recall_list.append(recall_score(y_train_crossval, train_predictions))
    test_recall_list.append(recall_score(y_test_crossval, test_predictions))
    train_precision_list.append(precision_score(y_train_crossval, train_predictions))
    test_precision_list.append(precision_score(y_test_crossval, test_predictions))
    train_auc_list.append(roc_auc_score(y_train_crossval, model.predict_proba(X_train_crossval)[:, 1]))
    test_auc_list.append(roc_auc_score(y_test_crossval, model.predict_proba(X_test_crossval)[:, 1]))

# Print average metrics across all folds for both training and testing
print('Average Training Metrics Across All Folds:')
print(f'Accuracy: {np.mean(train_accuracy_list)*100:.2f}%')
print(f'Recall: {np.mean(train_recall_list)*100:.2f}%')
print(f'Precision: {np.mean(train_precision_list)*100:.2f}%')
print(f'AUC: {np.mean(train_auc_list):.4f}')

print('Average Testing Metrics Across All Folds:')
print(f'Accuracy: {np.mean(test_accuracy_list)*100:.2f}%')
print(f'Recall: {np.mean(test_recall_list)*100:.2f}%')
print(f'Precision: {np.mean(test_precision_list)*100:.2f}%')
print(f'AUC: {np.mean(test_auc_list):.4f}')

"""After performing Stratified k-fold cross validation we observe that the testing precision is now very close to the training precision (around 68%). Stratified k-fold cross-validation provides more robust estimates of model performance compared to the simple train/test split we conducted above. However, this is not reflective of the real world where we expect the distribution of classes not to be balanced (meaning that the test set is unlikely to be balanced).

### **6.1.2: Undersampling to balance classes**
"""

# Concatenate features (X_train) and target variable (y_train) along the columns axis
data = pd.concat([X_train, y_train], axis=1)

# Separate by class
majority_class = data[data['Churn'] == 0]
minority_class = data[data['Churn'] == 1]

# Perform undersampling of the majority class
majority_class_undersampled = majority_class.sample(n=len(minority_class), random_state=42)

# Concatenate undersampled majority class with the minority class and shuffle
balanced_data = pd.concat([majority_class_undersampled, minority_class])
balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Split the balanced data back into features and target
X_train_balanced = balanced_data.drop('Churn', axis=1)
y_train_balanced = balanced_data['Churn']

# Initialize LogisticRegression
lrg = LogisticRegression()

# Fit the logisitc regression model on the training data
lrg.fit(X_train_balanced, y_train_balanced)

# Predict outcomes for the train and test sets
lrg_train_predictions = lrg.predict(X_train_balanced)
lrg_test_predictions = lrg.predict(X_test)

# Calculate accuracy, recall and precision metrics for train and test sets
lrg_train_acc = accuracy_score(y_train_balanced, lrg.predict(X_train_balanced))
lrg_test_acc = accuracy_score(y_test, lrg.predict(X_test))
lrg_train_rec = recall_score(y_train_balanced, lrg.predict(X_train_balanced))
lrg_test_rec = recall_score(y_test, lrg.predict(X_test))
lrg_train_pre = precision_score(y_train_balanced, lrg.predict(X_train_balanced))
lrg_test_pre = precision_score(y_test, lrg.predict(X_test))

print('Regular Logistic Regression Performance')
print('---------------------------------------')
print(f'Training Accuracy: {lrg_train_acc*100:.2f}%')
print(f'Testing Accuracy: {lrg_test_acc*100:.2f}%')
print(f'Training Recall: {lrg_train_rec*100:.2f}%')
print(f'Testing Recall: {lrg_test_rec*100:.2f}%')
print(f'Training Precision: {lrg_train_pre*100:.2f}%')
print(f'Testing Precision: {lrg_test_pre*100:.2f}%')

auc_train = roc_auc_score(y_train_balanced, lrg.predict_proba(X_train_balanced)[:, 1])
auc_test = roc_auc_score(y_test, lrg.predict_proba(X_test)[:, 1])

print('AUC Score')
print('---------')
print(f'Training AUC: {auc_train:.4f}')
print(f'Testing AUC: {auc_test:.4f}')

"""Undersampling did not result in any significant changes in the performance of our model. Some of the performance metrics were very slightly lowered relative to the logistic regression using the SMOTE-balanced dataset. Here as well, the testing precision(32%) is much lower than the training precision(68%). Both the training and the testing AUC are also very slightly lower than previously.

### **Part 6.1.3: Hyperparameter Tuning**

We have conducted hyperparameter tuning in order to optimize the performance of our model by searching for the best hyperparameters. A well-chosen metric needs to be picked for hyperparameter tuning. In our case of predicting churn, we were between using precision or recall (we ruled out accuracy given the inherent imbalance in the data). We realized that
*   Precision would be used when we would like to minimize the false positives, such as wasted resources on unnecessary customer retention efforts
*   Recall would be used when it would be critical to identify as many churners as possible (for example because the cost of losing a customer is very high)

Given that the dataset does not name the company, we assumed that Recall is more important in our case.

Additionally, our data is not overfitting to the training set, but we have decided to include the 'Regularization Strength' parameter, C, as the model might benefit from it. We have, however, used small values of C. Also, we have used the following parameters:
*   max_iter: Maximum number of iterations taken for the solvers to converge
"""

# Initialize Stratified K-Fold cross-validation object with 5 splits and shuffling
cv = StratifiedKFold(n_splits=5, shuffle=True)

# TODO: Define the Logistic Regression classification model/estimator
estimator = LogisticRegression(solver='saga', max_iter=1000)

# TODO: Define the hyperparameter search space/grid
param_grid = {
    'C': [0.01, 0.1],
    'max_iter': [100, 200, 300, 400, 500]
}

# TODO: Define the metric as a lower case string (for example: scoring = 'accuracy')
scoring = "recall"

# Initialize GridSearchCV with the defined estimator, parameter grid, scoring metric, cross-validation, and verbosity level
search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=scoring, cv=cv, verbose=2)

# TODO: Execute the grid search to find the best model
search.fit(X_train_with_clusters, y_train_smote)

# Get the best model from the search
best_model = search.best_estimator_

y_train_pred = best_model.predict(X_train_with_clusters)
y_test_pred = best_model.predict(X_test_with_clusters)

# Evaluate performance metrics on the modified training set
train_acc = accuracy_score(y_train_smote, y_train_pred)
train_recall = recall_score(y_train_smote, y_train_pred)
train_precision = precision_score(y_train_smote, y_train_pred)
train_auc = roc_auc_score(y_train_smote, best_model.predict_proba(X_train_with_clusters)[:, 1])

# Evaluate performance metrics on the testing set
test_acc_lrg = accuracy_score(y_test, y_test_pred)
test_recall_lrg = recall_score(y_test, y_test_pred)
test_precision_lrg = precision_score(y_test, y_test_pred)
test_auc_lrg = roc_auc_score(y_test, best_model.predict_proba(X_test_with_clusters)[:, 1])


# TODO: Compute recall score on the testing set using the best model
search_score = recall_score(y_test, y_test_pred)

# DO NOT CHANGE ----------------------------------------------------------------
print(f'The best Logistic Regression model has hyperparameters {search.best_params_}')
print(f'The best model achieves an average cross-validation score of {search.best_score_*100:.2f}%')
print(f'The best model achieves a score of {search_score*100:.2f}% on the testing data')
# DO NOT CHANGE ----------------------------------------------------------------

print()
# Print modified training set performance
print('Logistic Regression Performance on Modified Training Set')
print('--------------------------------------------------------')
print(f'Training Accuracy: {train_acc*100:.2f}%')
print(f'Training Recall: {train_recall*100:.2f}%')
print(f'Training Precision: {train_precision*100:.2f}%')
print(f'Training AUC: {train_auc:.4f}')

# Print testing set performance
print('Logistic Regression Performance on Test Set')
print('--------------------------------------------')
print(f'Testing Accuracy: {test_acc_lrg*100:.2f}%')
print(f'Testing Recall: {test_recall_lrg*100:.2f}%')
print(f'Testing Precision: {test_precision_lrg*100:.2f}%')
print(f'Testing AUC: {test_auc_lrg:.4f}')
# DO NOT CHANGE ----------------------------------------------------------------

"""Hyperparameter tuning did not result in significant changes in the performance metrics. This is because the logistic regression was not overfitting, so the regularization strength parameter did not substantially affect the model's accuracy or generalizability.

**Additional Thoughts**

To improve our logistic model, we also tried some additional feature engineering. For example, we tried generating interaction features between 'MonthlyCharges' and 'ContentDownloadsPerMonth' using polynomial transformation with degree 2. This would intuitively make sense because high monthly charges combined with low content downloads per month could lead to higher churn rates, or vice versa. However, adding the interaction terms significantly reduced the performance of the model.

## **6.2: Ensemble Models**

### **Part 6.2.1: XGBoost**

After examining the results of the logistic regression, we decided to implement XGBoost, which does not assume linearity and thus allows for interactions between features and greater flexibility. Additionally, we saw that logistic regression did not overfit, but rather underfit our data so we would like additional complexity to our models.

XGBoost works by sequentially adding decision trees to an ensemble, optimizing a specific objective function that combines a loss function and regularization terms to minimize prediction error and prevent overfitting.

Limitations: Some of the limitations of XGBoost include that it has limited interpretability as it does not provide exact relationships between the features and the target variable. Additionally, another limitation is that it is very sensitive to hyperparameters and so requires very sensitive hyperparameter tunning to prevent overfitting or underfitting.

Below is the implementation of XGBoost. For the XGBoost and the RandomForest, we didn't use the engineered feature we used in logistic regression. We saw that the performance didn't improve with the extra feature of cluster labels. This is potentially because of the greater degree of flexibility that these models allow and their ability to capture non-linear effects.

Given the size of our dataset (around 160000 rows), we arbitrarily chose the Random Forest to have a moderate number of trees (n_estimators = 100). Similarly, we started with a moderate maximum depth of trees (max depth = 5) and learning rate (learning_rate = 0.1).
"""

# Initialize the XGBoost classifier
xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)

# Fit the model on the training data
xgb.fit(X_train_smote, y_train_smote)

# Predict on the test data
xgb_test_predictions = xgb.predict(X_test)
xgb_train_predictions = xgb.predict(X_train_smote)
xgb_test_probabilities = xgb.predict_proba(X_test)[:, 1]

# Evaluate the model using accuracy, precision, recall, F1-score, and AUC
xgb_train_accuracy = accuracy_score(y_train_smote, xgb_train_predictions)
xgb_test_accuracy = accuracy_score(y_test, xgb_test_predictions)
xgb_train_precision = precision_score(y_train_smote, xgb_train_predictions)
xgb_test_precision = precision_score(y_test, xgb_test_predictions)
xgb_train_recall = recall_score(y_train_smote, xgb_train_predictions)
xgb_test_recall = recall_score(y_test, xgb_test_predictions)
xgb_train_f1 = f1_score(y_train_smote, xgb_train_predictions)
xgb_test_f1 = f1_score(y_test, xgb_test_predictions)
xgb_train_auc = roc_auc_score(y_train_smote, xgb_train_predictions)
xgb_test_auc = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])

# Print the evaluation results
print("XGBoost Model Performance")
print("Training Accuracy: {:.2f}%".format(xgb_train_accuracy * 100))
print("Test Accuracy: {:.2f}%".format(xgb_test_accuracy * 100))
print("Training Precision: {:.2f}%".format(xgb_train_precision * 100))
print("Test Precision: {:.2f}%".format(xgb_test_precision * 100))
print("Training Recall: {:.2f}%".format(xgb_train_recall * 100))
print("Test Recall: {:.2f}%".format(xgb_test_recall * 100))
print("Training F1 Score: {:.2f}".format(xgb_train_f1))
print("Test F1 Score: {:.2f}".format(xgb_test_f1))
print("Train AUC Score: {:.4f}".format(xgb_train_auc))
print("Test AUC Score: {:.4f}".format(xgb_test_auc))

"""We observe that model performance in the train set significantly increased from the logistic regression model. For example, Training Recall increased from 70.57% to 80.78%. However, the performance metrics in the test set are significantly lower than in the logistic regression model. For example, the Testing Recall decreased from 68.51% to 17.09%. This is a very big decrease which suggests very significant overfitting. Part of the reason for this significant overfitting is the fact that while we balanced the train set, we did not balance the test set. Therefore, the model has learned to make predictions without being biased towards the majority class.

### **Part 6.2.2: Hyperparameter Tunning**

Given that the XGBoost model overfits, we will need to select a set of hyperparameters that will reduce overfitting. We decided on the following parameters:


*   max_depth: Controls the maximum depth of each tree. Lower values restrict the depth of the tree and reduce overfitting, so we tried values in [3,5]
*   learning_rate: Scales the contribution of each tree. We tested lower values than our initial model. The values included [0.01, 0.001]
*   n_estimators: Number of trees in the model. We tested low values, which were [50,100]
"""

# Initialize XGBoost classifier
xgb = XGBClassifier(random_state=42)

# Define the parameter grid
param_grid = {
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.001],
    'n_estimators': [50, 100],
}

# Setup the GridSearchCV
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='recall', cv=3, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train_smote, y_train_smote)

# Best parameters and best model
print("Best parameters found: ", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Predictions with the best model
train_predictions = best_model.predict(X_train_smote)
test_predictions = best_model.predict(X_test)
train_proba = best_model.predict_proba(X_train_smote)[:, 1]
test_proba = best_model.predict_proba(X_test)[:, 1]

# Evaluation
train_accuracy = accuracy_score(y_train_smote, train_predictions)
train_precision = precision_score(y_train_smote, train_predictions)
train_recall = recall_score(y_train_smote, train_predictions)
train_f1 = f1_score(y_train_smote, train_predictions)
train_auc = roc_auc_score(y_train_smote, train_proba)

test_accuracy_xgboost = accuracy_score(y_test, test_predictions)
test_precision_xgboost = precision_score(y_test, test_predictions)
test_recall_xgboost = recall_score(y_test, test_predictions)
test_f1_xgboost = f1_score(y_test, test_predictions)
test_auc_xgboost = roc_auc_score(y_test, test_proba)

# Print performance metrics
print("XGBoost Model Performance with Best Parameters")
print("Training Accuracy: {:.2f}%".format(train_accuracy * 100))
print("Test Accuracy: {:.2f}%".format(test_accuracy_xgboost * 100))
print("Training Precision: {:.2f}%".format(train_precision * 100))
print("Test Precision: {:.2f}%".format(test_precision_xgboost * 100))
print("Training Recall: {:.2f}%".format(train_recall * 100))
print("Test Recall: {:.2f}%".format(test_recall_xgboost * 100))
print("Training F1 Score: {:.2f}".format(train_f1))
print("Test F1 Score: {:.2f}".format(test_f1_xgboost))
print("Training AUC Score: {:.4f}".format(train_auc))
print("Test AUC Score: {:.4f}".format(test_auc_xgboost))

"""By using hyperparameter tunning we managed to significantly lower the gap between the train and test metrics. However, in doing so, the performance metrics in both the train and the test set decreased. For example, the Training Precision dropped from 96.12% to 63.35% after hyperparameter tuning. Also, the Test Precision dropped from 51.61% to 26.86%. The model is still overfitting as the difference between the train and test metrics is present. The AUC of the train and test set are not too far from each other. The test set has an AUC score of 0.6828, which indicates that the performance is not too bad.

### **Part 6.2.3: scale_pos_weight parameter**

Considering that the XGBoost model is not performing well because of the imbalance in the test set, we will try to use the scale_pos_weight parameter instead of using the SMOTE-balanced train dataset. This parameter helps to adjust the weight of the positive class relative to the negative class. More specifically, it adjusts the cost function used by the model without altering the original distribution. We therefore expect that it will perform better on the test set.
"""

# Initialize the XGBoost classifier with scale_pos_weight parameter
xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train),  # Calculate scale_pos_weight
    random_state=42
)

# Fit the model on the training data
xgb.fit(X_train, y_train)

# Predict on the test data
xgb_test_predictions = xgb.predict(X_test)
xgb_train_predictions = xgb.predict(X_train)
xgb_test_probabilities = xgb.predict_proba(X_test)[:, 1]

# Evaluate the model using accuracy, precision, recall, F1-score, and AUC
xgb_train_accuracy = accuracy_score(y_train, xgb_train_predictions)
xgb_test_accuracy = accuracy_score(y_test, xgb_test_predictions)
xgb_train_precision = precision_score(y_train, xgb_train_predictions)
xgb_test_precision = precision_score(y_test, xgb_test_predictions)
xgb_train_recall = recall_score(y_train, xgb_train_predictions)
xgb_test_recall = recall_score(y_test, xgb_test_predictions)
xgb_train_f1 = f1_score(y_train, xgb_train_predictions)
xgb_test_f1 = f1_score(y_test, xgb_test_predictions)
xgb_train_auc = roc_auc_score(y_train, xgb_train_predictions)
xgb_test_auc = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])

# Print the evaluation results
print("XGBoost Model Performance")
print("Training Accuracy: {:.2f}%".format(xgb_train_accuracy * 100))
print("Test Accuracy: {:.2f}%".format(xgb_test_accuracy * 100))
print("Training Precision: {:.2f}%".format(xgb_train_precision * 100))
print("Test Precision: {:.2f}%".format(xgb_test_precision * 100))
print("Training Recall: {:.2f}%".format(xgb_train_recall * 100))
print("Test Recall: {:.2f}%".format(xgb_test_recall * 100))
print("Training F1 Score: {:.2f}".format(xgb_train_f1))
print("Test F1 Score: {:.2f}".format(xgb_test_f1))
print("Train AUC Score: {:.4f}".format(xgb_train_auc))
print("Test AUC Score: {:.4f}".format(xgb_test_auc))

"""By using the scale_pos_weight parameter instead of the SMOTE-balanced train dataset, we were able to decrease the difference between the train and test set metrics. Now the Test Recall increased from 17.09% to 69.26%. Both the Test Precision and Accuracy decreased. However, we now have more consistent results. There is still some slight overfitting due to the slightly greater metrics of the train set. It is interesting to note, however, that the Train AUC is lower than the Test AUC. This is probably because the AUC is more sensitive to the ranking of predicted probabilities rather than their absolute values. The F1 metric on the other hand is slightly lower for the Test set.

### **Part 6.2.4: Hyperparameter Tuning using scale_pos_weight**

We proceeded by conducting hyperparameter tuning on the XGBoost model that used the scale_pos_weight parameter. Given that we didn't see significant overfitting in the model, we have selected greater values for the hyperparameters. These are max_depth: [5, 10], learning_rate: [0,1, 0.01, 0.001], n_estimators: [100, 200].
"""

# Initialize XGBoost classifier
xgb = XGBClassifier(random_state=42)

# Calculate the scale_pos_weight
scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)

# Define the parameter grid
param_grid = {
    'max_depth': [5, 10],
    'learning_rate': [0,1, 0.01, 0.001],
    'n_estimators': [100, 200],
    'scale_pos_weight': [scale_pos_weight]  # Add this directly since it's a specific value from data
}

# Setup the GridSearchCV
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='recall', cv=3, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Best parameters and best model
print("Best parameters found: ", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Predictions with the best model
train_predictions = best_model.predict(X_train)
test_predictions = best_model.predict(X_test)
train_proba = best_model.predict_proba(X_train)[:, 1]
test_proba = best_model.predict_proba(X_test)[:, 1]

# Evaluation
train_accuracy = accuracy_score(y_train, train_predictions)
train_precision = precision_score(y_train, train_predictions)
train_recall = recall_score(y_train, train_predictions)
train_f1 = f1_score(y_train, train_predictions)
train_auc = roc_auc_score(y_train, train_proba)

test_accuracy = accuracy_score(y_test, test_predictions)
test_precision = precision_score(y_test, test_predictions)
test_recall = recall_score(y_test, test_predictions)
test_f1 = f1_score(y_test, test_predictions)
test_auc = roc_auc_score(y_test, test_proba)

# Print performance metrics
print("XGBoost Model Performance with Best Parameters")
print("Training Accuracy: {:.2f}%".format(train_accuracy * 100))
print("Test Accuracy: {:.2f}%".format(test_accuracy * 100))
print("Training Precision: {:.2f}%".format(train_precision * 100))
print("Test Precision: {:.2f}%".format(test_precision * 100))
print("Training Recall: {:.2f}%".format(train_recall * 100))
print("Test Recall: {:.2f}%".format(test_recall * 100))
print("Training F1 Score: {:.2f}".format(train_f1))
print("Test F1 Score: {:.2f}".format(test_f1))
print("Training AUC Score: {:.4f}".format(train_auc))
print("Test AUC Score: {:.4f}".format(test_auc))

"""Implementing hyperparameter tuning in the model that uses the scale_pos_weight parameter has further reduced the overfitting that was observed. The train and test performance metrics are now closer to each other. However, it should be noted that the results are very similar to results obtained from the logistic regression, indicating that the performance of XGBoost was not better."""

# Get feature importances
feature_importances = best_model.feature_importances_

# Sort the feature importances in descending order and get the indices
sorted_indices = np.argsort(feature_importances)[::-1]

# Create labels for the sorted feature importances
sorted_labels = [X_train_smote.columns[i] for i in sorted_indices]

# Plot
plt.figure(figsize=(10, 8))
plt.barh(range(len(feature_importances)), feature_importances[sorted_indices], align='center')
plt.yticks(range(len(feature_importances)), sorted_labels)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()  # Invert the y-axis to have the highest importance at the top
plt.title('Sorted Feature Importance from XGBoost Model')
plt.show()

"""Looking at the Feature Importances of the XGBoost model, we made the following observations:
*   AccountAge is the most significant variable in predicting churn. This also aligns with the results of the logistic regression, where AccountAge had the greatest coefficient in magnitude.
*   AverageViewingDuration is the second most important feature in the XGBoost model. This was also the feature with the second-highest coefficient in the logistic regression.
*   The next two features, ViewingHoursPerWeek and ContentDownloadsPerMonth also agree with the results of the logistic regression.
*   It is interesting that SubscriptionType_Premium appears among the most significant features. As we showed in our EDA analysis, SubscriptionType_Premium customers who churned were significantly fewer than the basic and standard subscription types. The results therefore confirm our initial business insight that subscription type predicts churn.
*   Finally, we observe that many of the features have 0 significance. This means that they were not used in any of the model iterations to make predictions.

### **Part 6.3: Random Forests**

Based on the overfitting of XGBoost, we next decided to implement a Random Forest. Random forests are effective in mitigating overfitting because they combine multiple decision trees, each trained on a subset of the data and features, and aggregate their predictions, which helps reduce variance and improve generalization to unseen data. Additionally, Random forests are beneficial as they implicitly perform feature selection by selecting a random subset of features for each tree. This process helps prevent overfitting by reducing the reliance on any single feature.

Limitations: The main limitation of Random Forests is that it lacks interpretability. Feature importance will give us a sense of how each feature contributes to the model's performace. However, they are only relative and might not capture the nuances of non-linear importance or interactions between features.

Given the size of our dataset (around 160000 rows), we arbitrarily chose the Random Forest to have a moderate number of trees (numTrees = 100). Similarly we started with a moderate maximum depth of trees (maxDepth = 5).

Our dataset is large with 243787 rows and 21 columns. Given the size and complexity, we expect that the implementation of a more extensive hyperparameter tuning in a machine-learning model could encounter computational challenges, extended processing times, and potentially memory constraints using our single-node setup. Therefore, we will use Apache Spark. This will allow for distributed processing, speed up the learning phase, and allow us to build better models. Specifically, we will use the MLlib library, which provides many tools we will use.

### **Part 6.3.1: Spark**
"""

# Install PySpark and FindSpark packages
!pip install pyspark
!pip install findspark

# Import FindSpark to locate Spark installation
import findspark
# Initialize FindSpark to locate Spark in the system environment
findspark.init()
# Import SparkSession to create Spark application
from pyspark.sql import SparkSession

# Create a SparkSession with local[*] as master and 'Basics' as app name
spark = SparkSession.builder \
        .master('local[*]') \
        .appName('Basics') \
        .getOrCreate()

# Download dataset from the provided URL and save it as RAW_recipes_cleaned.csv
!wget http://raw-recipes-clean-upgrad.s3.amazonaws.com/RAW_recipes_cleaned.csv

"""**Relevant Imports**"""

# Import necessary modules for building and evaluating Spark machine learning pipelines
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator
from pyspark.mllib.evaluation import MulticlassMetrics
from sklearn.metrics import roc_auc_score
from pyspark.mllib.evaluation import BinaryClassificationMetrics
from pyspark.sql.functions import col
from pyspark.ml.tuning import ParamGridBuilder
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Copy the SMOTE-augmented training feature DataFrame to a new DataFrame.
X_train_smote_rf = X_train_smote.copy()

# Add the target variable 'Churn' from the SMOTE-augmented training labels to the new DataFrame.
X_train_smote_rf['Churn'] = y_train_smote

# Convert the Pandas DataFrame that now includes features and target variable to a Spark DataFrame.
train_sdf = spark.createDataFrame(X_train_smote_rf)

# Copy the test feature DataFrame to a new DataFrame to avoid modifications to the original test dataset.
X_test_rf = X_test.copy()

# Append the target variable 'Churn' to the new test DataFrame.
X_test_rf['Churn'] = y_test

# Convert the modified test Pandas DataFrame to a Spark DataFrame.
test_sdf = spark.createDataFrame(X_test_rf)

train_sdf.show()

# Convert the 'Churn' column in the Spark DataFrame to double type
train_sdf = train_sdf.withColumn("Churn", col("Churn").cast("double"))
test_sdf = test_sdf.withColumn("Churn", col("Churn").cast("double"))

"""**Setting-up Vector Assembler**

Before we continue with the implementation of models using ApacheSpark, we have to create a vector assembler, which combines a given list of columns into a single vector column. This is necessary to be able to feed ML algorithms in Spark's MLlib.
"""

# Create a list of feature names, excluding the target variable 'Churn'
feature_columns = [col for col in train_sdf.columns if col != 'Churn']

# Initialize the VectorAssembler with the feature columns
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

# Create a Pipeline with the VectorAssembler as a stage
pipeline = Pipeline(stages=[assembler])

# Fit the pipeline to the data and transform the dataframe to include the new features column
train_sdf = pipeline.fit(train_sdf).transform(train_sdf)
test_sdf = pipeline.fit(test_sdf).transform(test_sdf)

"""### **Part 6.3.2: Random Forest Implementation**"""

# Fit the RandomForest model on the train set
rf = RandomForestClassifier(labelCol='Churn', featuresCol='features', numTrees=100, maxDepth=5)

# Train the RandomForest model on the training DataFrame and obtain the fitted model
rf_model = rf.fit(train_sdf)

# Make predictions on the test data
test_predictions = rf_model.transform(test_sdf)

# Evaluate the model's performance
evaluator = BinaryClassificationEvaluator(labelCol="Churn")

# Prepare the test data predictions and true labels for evaluation
test_true_and_pred_rdd = test_predictions.select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of predictions and true labels
metrics = MulticlassMetrics(test_true_and_pred_rdd)

# Extract the confusion matrix from the MulticlassMetrics
confusion_matrix_rf = metrics.confusionMatrix().toArray()

# Calculate percentages
total_instances = sum(confusion_matrix_rf.flatten())
confusion_matrix_percentages = (confusion_matrix_rf / total_instances) * 100

# Calculate training accuracy
training_accuracy_rf = rf_model.summary.accuracy

# Prepare the train data predictions and true labels for evaluation
train_true_and_pred_rdd = rf_model.transform(train_sdf).select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of train predictions and true labels
train_metrics = MulticlassMetrics(train_true_and_pred_rdd)

# Calculate accuracy, recall, and precision for train set
train_accuracy = train_metrics.accuracy
train_recall = train_metrics.recall(label=1.0)
train_precision = train_metrics.precision(label=1.0)

# Prepare the test data predictions and true labels for evaluation
test_true_and_pred_rdd = rf_model.transform(test_sdf).select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of test predictions and true labels
test_metrics = MulticlassMetrics(test_true_and_pred_rdd)

# Calculate accuracy, recall, and precision for test set
test_accuracy = test_metrics.accuracy
test_recall = test_metrics.recall(label=1.0)
test_precision = test_metrics.precision(label=1.0)

# Calculate AUC for train set
train_auc_rf = evaluator.evaluate(rf_model.transform(train_sdf))

# Calculate AUC for test set
test_auc_rf = evaluator.evaluate(test_predictions)

# Print performance metrics
print('Random Forest Performance on Training Set')
print('-----------------------------------------')
print(f'Training Accuracy: {train_accuracy*100:.2f}%')
print(f'Training Recall: {train_recall*100:.2f}%')
print(f'Training Precision: {train_precision*100:.2f}%')
print(f'Training AUC: {train_auc_rf:.4f}\n')

print('Random Forest Performance on Test Set')
print('-------------------------------------')
print(f'Testing Accuracy: {test_accuracy*100:.2f}%')
print(f'Testing Recall: {test_recall*100:.2f}%')
print(f'Testing Precision: {test_precision*100:.2f}%')
print(f'Testing AUC: {test_auc_rf:.4f}\n')

print('Confusion Matrix Percentages:')
print(confusion_matrix_percentages)

"""As expected, our Random Forest exhibits less overfitting than the XGBoost model, since the training and test metrics are closer to each other. The performance of the train set is only slightly better than the logistic regression. The test set performance is slightly worse than the logistic regression. More specifically, although accuracy and precision are about the same, recall has fallen from 68.51% to 61.42%.
The AUC (0.7154) still indicates that the performance of the model is not too bad.

We hypothesize that there is mild overfitting, meaning that the Random Forest model may be capturing noise in the training data rather than generalizing well to unseen data. The reason we suggest that overfitting is mild is because the imbalance in the test set significantly affects results.
"""

# Copy the training features to a new DataFrame to prevent modifying the original data
train_sdf_imbalance = X_train.copy()

# Add the target variable 'Churn' from the training labels to the DataFrame
train_sdf_imbalance['Churn'] = y_train

# Convert the Pandas DataFrame to a Spark DataFrame for distributed processing
train_sdf_imbalance = spark.createDataFrame(train_sdf_imbalance)

# Ensure the 'Churn' column is of type double to comply with the requirements of many ML algorithms in Spark
train_sdf_imbalance = train_sdf_imbalance.withColumn("Churn", col("Churn").cast("double"))

# Apply a predefined pipeline of transformations to the Spark DataFrame. This may include steps like
# vectorizing features, scaling, or other preprocessing needed before fitting a model
train_sdf_imbalance = pipeline.fit(train_sdf_imbalance).transform(train_sdf_imbalance)

"""In order to find the best random forest classifier, we used hyperparameter tunning. The following are the parameters we chose to optimize:


*   maxDepth: maximum depth of each decision tree in the Random Forest. Our initial Random Forest had a maxDepth = 5, so we will keep this as is, given that we do not want to increase it.
*   numTrees: number of trees to be used in the Random Forest. We will try the values [5,10] in hope of reducing overfitting.

### **Part 6.3.2: Hyperparameter Tunning**
"""

# Initialize the RandomForestClassifier
rf = RandomForestClassifier(labelCol='Churn')

# Build a parameter grid for model tuning
param_grid = ParamGridBuilder() \
    .addGrid(rf.maxDepth, [5]) \
    .addGrid(rf.numTrees, [5, 10]) \
    .build()

# Initialize the MulticlassClassificationEvaluator for recall
evaluator = MulticlassClassificationEvaluator(labelCol="Churn",
                                              metricName="weightedRecall")  # Average recall over all classes

# Set up cross-validation with the RandomForest estimator
crossval = CrossValidator(estimator=rf,
                          estimatorParamMaps=param_grid,
                          evaluator=evaluator,
                          numFolds=3)

# Fit the cross-validation model on the train set
cv_model = crossval.fit(train_sdf)

# Extract the best model from the cross-validation
best_rf_model = cv_model.bestModel

# Calculate training accuracy
training_accuracy_rf = best_rf_model.summary.accuracy

# Prepare the train data predictions and true labels for evaluation
train_predictions = best_rf_model.transform(train_sdf)

# Prepare the train data predictions and true labels for evaluation
train_true_and_pred_rdd = train_predictions.select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of train predictions and true labels
train_metrics = MulticlassMetrics(train_true_and_pred_rdd)

# Calculate accuracy, recall, and precision for train set
train_accuracy_rf = train_metrics.accuracy
train_recall_rf = train_metrics.recall(label=1.0)
train_precision_rf = train_metrics.precision(label=1.0)

# Transform the test dataset using the best model to get predictions
test_predictions = best_rf_model.transform(test_sdf)

# Prepare the test data predictions and true labels for evaluation
test_true_and_pred_rdd = test_predictions.select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of test predictions and true labels
test_metrics = MulticlassMetrics(test_true_and_pred_rdd)

# Calculate accuracy, recall, and precision for test set
test_accuracy_rf = test_metrics.accuracy
test_recall_rf = test_metrics.recall(label=1.0)
test_precision_rf = test_metrics.precision(label=1.0)

# Extract the confusion matrix from the MulticlassMetrics
confusion_matrix_rf = test_metrics.confusionMatrix().toArray()

# Calculate percentages
total_instances = sum(confusion_matrix_rf.flatten())
confusion_matrix_percentages = (confusion_matrix_rf / total_instances) * 100

# Calculate AUC for training set
train_auc_rf = evaluator.evaluate(train_predictions)

# Calculate AUC for test set
test_auc_rf = evaluator.evaluate(test_predictions)

# Print performance metrics
print('Random Forest Performance on Training Set')
print('-----------------------------------------')
print(f'Training Accuracy: {train_accuracy_rf*100:.2f}%')
print(f'Training Recall: {train_recall_rf*100:.2f}%')
print(f'Training Precision: {train_precision_rf*100:.2f}%')
print(f'Training AUC: {train_auc_rf:.4f}\n')

print('Random Forest Performance on Test Set')
print('-------------------------------------')
print(f'Testing Accuracy: {test_accuracy_rf*100:.2f}%')
print(f'Testing Recall: {test_recall_rf*100:.2f}%')
print(f'Testing Precision: {test_precision_rf*100:.2f}%')
print(f'Testing AUC: {test_auc_rf:.4f}\n')

print('Confusion Matrix Percentages:')
print(confusion_matrix_percentages)

"""The hyperparameter tuning did not significantly result in the improvement of the model performance. Both the metrics are relatively the same with the initial model.

### **Part 6.3.3: Built-in class Balance**

As we did with the scale_pos_weight parameter in the XGBoost model, we will use the built-in weightCol parameter in the RandomForest as an alternative way of dealing with the class imbalance. In this way, the original distribution will be preserved.
"""

from pyspark.sql.functions import when
class_counts = train_sdf_imbalance.groupBy('Churn').count()
total_count = train_sdf_imbalance.count()
class_weights = {row['Churn']: total_count / row['count'] for row in class_counts.collect()}


# Step 2: Create a new column for weights based on class labels
train_sdf_weighted = train_sdf_imbalance.withColumn('weight', when(train_sdf_imbalance['Churn'] == 0, class_weights[0]).otherwise(class_weights[1]))
# Fit the RandomForest model on the train set with class weights
rf = RandomForestClassifier(labelCol='Churn', featuresCol='features', numTrees=100, maxDepth=5, weightCol='weight')

rf_model = rf.fit(train_sdf_weighted)

# Make predictions on the test data
test_predictions = rf_model.transform(test_sdf)

# Evaluate the model's performance
evaluator = BinaryClassificationEvaluator(labelCol="Churn")

# Prepare the test data predictions and true labels for evaluation
test_true_and_pred_rdd = test_predictions.select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of predictions and true labels
metrics = MulticlassMetrics(test_true_and_pred_rdd)

# Extract the confusion matrix from the MulticlassMetrics
confusion_matrix_rf = metrics.confusionMatrix().toArray()

# Calculate percentages
total_instances = sum(confusion_matrix_rf.flatten())
confusion_matrix_percentages = (confusion_matrix_rf / total_instances) * 100

# Calculate training accuracy
training_accuracy_rf = rf_model.summary.accuracy

# Prepare the train data predictions and true labels for evaluation
train_true_and_pred_rdd = rf_model.transform(train_sdf_weighted).select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of train predictions and true labels
train_metrics = MulticlassMetrics(train_true_and_pred_rdd)

# Calculate accuracy, recall, and precision for train set
train_accuracy = train_metrics.accuracy
train_recall = train_metrics.recall(label=1.0)
train_precision = train_metrics.precision(label=1.0)

# Prepare the test data predictions and true labels for evaluation
test_true_and_pred_rdd = rf_model.transform(test_sdf).select(col("prediction"), col("Churn").cast("double")).rdd.map(tuple)

# Initialize MulticlassMetrics with the RDD of test predictions and true labels
test_metrics = MulticlassMetrics(test_true_and_pred_rdd)

# Calculate accuracy, recall, and precision for test set
test_accuracy = test_metrics.accuracy
test_recall = test_metrics.recall(label=1.0)
test_precision = test_metrics.precision(label=1.0)

# Calculate AUC for train set
train_auc_rf = evaluator.evaluate(rf_model.transform(train_sdf_weighted))

# Calculate AUC for test set
test_auc_rf = evaluator.evaluate(test_predictions)

# Print performance metrics
print('Random Forest Performance on Training Set')
print('-----------------------------------------')
print(f'Training Accuracy: {train_accuracy*100:.2f}%')
print(f'Training Recall: {train_recall*100:.2f}%')
print(f'Training Precision: {train_precision*100:.2f}%')
print(f'Training AUC: {train_auc_rf:.4f}\n')

print('Random Forest Performance on Test Set')
print('-------------------------------------')
print(f'Testing Accuracy: {test_accuracy*100:.2f}%')
print(f'Testing Recall: {test_recall*100:.2f}%')
print(f'Testing Precision: {test_precision*100:.2f}%')
print(f'Testing AUC: {test_auc_rf:.4f}\n')

print('Confusion Matrix Percentages:')
print(confusion_matrix_percentages)

"""Based on the above results we have managed to remove the overfitting that was taking place when using the SMOTE-oversampled train dataset. However, the testing and training metrics have generally decreased relative to the original Random Forest that used the SMOTE-balanced train set.

### **Part 6.3.4: Feature Importance**
"""

feature_importance = best_rf_model.featureImportances.toArray()

sorted_features = sorted(zip(feature_columns, feature_importance), key=lambda x: x[1], reverse=True)

# Unpacking the sorted features
sorted_feature_names, sorted_importances = zip(*sorted_features)

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_importances)), sorted_importances, align='center', color='steelblue')
plt.yticks(range(len(sorted_importances)), sorted_feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance at the top
plt.title('Feature Importance of Random Forest Model')
plt.show()

"""Looking at the feature importances we made the following observations:
*   The feature with the greatest importance is AccountAge. It is interesting to note that it has a much greater importance relative to the other features. The coefficient of the logistic regression for AccountAge is negative so we know that the greater the AccountAge, the lowest the probability of churn is. This is what we also concluded using the XGBoost model.
*   AverageViewingDuration was the feature with the second greatest importance. This aligns with the results of XGBoost.
*   As with XGBoost, we also observe that not all features have been used in training the Random Forest.

### **Part 6.3.5: Comparing Models**
"""

models = ['Logistic Regression', 'XGBoost', 'RandomForest']

# Assuming test_recall_lrg, test_recall_xgboost, test_recall_rf are defined earlier
# and are expressed as proportions (e.g., 0.75 for 75%)
recalls = [test_recall_lrg, test_recall_xgboost, test_recall_rf]

# Creating the bar plot
plt.figure(figsize=(8, 5))  # Set the figure size
bars = plt.bar(models, recalls, color=['blue', 'green', 'red'])  # Plot bars with different colors

# Adding title and labels
plt.title('Comparison of Model Recall Scores')
plt.xlabel('Models')
plt.ylabel('Recall')

# Adding percentage labels on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval*100:.2f}%', ha='center', va='bottom')

# Display the plot
plt.show()

"""The above plot enables us to compare the three models (after hyperparameter tuning) based on our metric of interest, recall. We observe that the RandomForest model has a significantly lower test recall compared to both the Logistic Regression and the XGBoost models. The Logistic Regression and XGBoost models have approximately equal test recall. Given our goal of interpreting our model and understanding how features contribute to churn, we would choose the Logistic Regression model over the XGBoost model.

### **Additional Remarks**

We decided not to implement a more complex model such as a NN. This is because we realized that the more complex the model was the less able it was to handle the class imbalance. In other words, as the model became more complex, it overfitted to the balanced train set we had created and could not generalize to an imbalanced data set (which is the test set found in the real world).
Additionally, we realized that the ensemble methods we performed (Random Forests and XGBoost) did not perform better than the logistic regression model. Logistic regression also has the additional benefit that it is more interpretable than the other models we implemented as well as more complex models, such as NN. This was an important factor to consider in our project as our ultimate objective was to understand the features that better explain churn.

# **Part 7: Conclusion**

**Main Takeaways:**
Our objective was to create a predictive model about whether a customer will churn or not based on many features describing their behavior. We were able to construct three models (Logistic Regression, XGBoost, and Random Forest). Below is the comparison of the models



*   Logistic Regression

Testing Accuracy: 68.14% /
Testing Recall: 68.51% /
Testing Precision: 32.11% /
Testing AUC: 0.7476

*   XGBoost

Test Accuracy: 60.58% /
Test Recall: 68.57% /
Test Precision: 26.86% /
Test AUC Score: 0.6828


*   RandomForest

Testing Accuracy: 67.69% /
Testing Recall: 61.83% /
Testing Precision: 30.54% /
Testing AUC: 0.6769


As we saw in the comparison of the recall scores graph, our model choice is Logistic Regression. It achieves the highest Test Recall, which is the metric of interest to us, and it is also interpretable which serves our ultimate objective of understanding the significance of different features in the churn rate of customers.


 We concluded the following for our ultimate objective:
*   AccountAge was the most important variable in all the models we used (highest coefficient in logistic regression, highest feature importance in ensemble models). We can therefore conclude that it is one of the most significant features in predicting the churn rate of customers. More specifically, as AccountAge increases, the probability of churn decreases. This makes sense as customers who have been with the service for a longer time are likely more satisfied and less inclined to churn.
*   Among the highest-importance features in all of the models were the SupportTicketsPerMonth, the AverageViewingDuration, and the SubscriptionType_Premium. This makes sense as these features directly reflect customer engagement and satisfaction.
*   The features that were least predictive of churn were the GenrePreference and the DeviceRegistered variables.

**Limitations:**

The most significant limitation of our models was the imbalanced dataset, which resulted in considerable trade-offs between accuracy metrics such as precision and recall. Additionally, this imbalance likely diminished the effectiveness of ensemble methods in predicting churn.

**Future work:**

*   One of the main areas of future work would be to further investigate class imbalance. This was one of the main challenges we faced in our project. To investigate class imbalance we could experiment with adjusting the threshold after training to cater to the minority class or better adjust the weights.
*   Additionally, we could also perform cross-validation across different datasets to verify the effectiveness and robustness of our models.
*   Our dataset did not have specific information about geography or time and as it was anonymous we found it difficult to merge with another dataset for additional information. However, it would be very interesting to examine how external factors like economic conditions or geographic data affect churn rates.
*   Finally, our topic allows for a lot of analysis on customer segmentation. For example, psychographic segmentation.

In terms of the next plans from a **business perspective**:

*   Based on the predictors such as 'AccountAge', 'SupportTicketsPerMonth', and 'SubscriptionType_Premium' develop targeted loyalty programs that incentivize long-term engagement.
*   Implement proactive support intervention based on the significance of 'SupportTicketsPerMonth' and customize the viewing experience to further enhance the 'AverageViewingDuration'
"""